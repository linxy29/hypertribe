# Checkpoint!
# What we have now: 
# - An excel sheet of counts of ref and alt alleles for each RNA edit site, across all 9 samples

# What do we want to know: 
# - Which edit sites are TRUE edit sites (i.e. our fusion protein generated a mismatch there)?
# - Which edit sites are NOISY/FALSE edit sites (i.e. also present in control; may have baseline alt allele count)?

# How do we do this? 
# - We need to make some sort of comparison of the edit frequencies between the fusion protein samples, and the control samples. 
# - If they are different enough, we can say that those edit sites are indeed TRUE edit sites, caused by our fusion protein. 
# - If they are not different enough between fusion protein and control samples, we likely say that those edit sites are probably FALSE edit sites (i.e. not generated by our fusion protein)

# ---> This script can help tell us if the edit frequencies are "different enough" between our FP samples and controls.

# We first begin by trying to model our edit frequencies after a certain statistical distribution:

#################################
# 1. Beta-binomial distribution
#################################
# The beta-binomial distribution is a family of discrete probability distributions on a finite support of non-negative integers arising when the probability of success in each of a fixed or known number of Bernoulli (Bernoulli distribution is the discrete probability distribution of a random variable which takes the value 1 with probability p and the value 0 with probability) trials is either unknown or random.

# The beta-binomial distribution is the binomial distribution in which the probability of success at each of n trials is not fixed but randomly drawn from a beta distribution.

# Has 3 parameters: n (no. of trials), alpha and beta (a parameter is any measured quantity of a statistical population that summarises or describes an aspect of the population, such as a mean or a standard deviation), where alpha > 0 and beta > 0

### Why beta-binomial distribution?
# Single parameter distributions, such as Poisson and Binomial, imply that the variance is determined by the mean.

# However in many cases and especially in analysis of biological data, this mean-variance relationship fails mainly due to presence of overdispersion, where the data have a higher variance than anticipated under the simple model. 

# In biology, overdispersion occurs since there is more than one source of variation: technical variation due to error measurements coming from the experiment design and biological variation between the subjects of interest, e.g. different cells on different conditions.

# By using beta-binomial, we allow each edit site to be independent of each other, thus having a different probability of success (i.e. A-G edit). We can then account for overdispersion when the probabilities of success are thought to vary from edit to edit.

# A good read that explains the use of beta-binomial distribution to model overdispersion 
# https://rpubs.com/cakapourani/beta-binomial

# We then want to estimate the parameters of our beta-binomial distribution:

##################################
# 2. Maximum likelihood estimation
##################################
# MLE is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. (i.e. given a set of observations/data, what parameters best explains the data --> returns the highest probability for the observed data?). Intuitively, it selects the parameter values that make the observed data most probable.

# The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.

# Often convenient to work with the natural logarithm of the likelihood function, called the log-likelihood (easier to work with logs, especially for exponential distributions)



# Libraries required
if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("bbmle")
if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("statmod")
library(bbmle)
library(statmod)
library(openxlsx)
library(parallel)
library(stringr)
library(VGAM)
library(DEXSeq)
library(tidyverse)

# Read in excel file of snp counts
setwd("/home/linxy29/data/TRIBE/HGC20240511005-0002/extract_RNAedit")
df_snp_counts <- read.xlsx( "snp_counts_dedupped.xlsx", sheet = 1 )

##################################
# Generating our model for null hypothesis
# MLE for our Null hypothesis: 
# H0: "All samples have equal RNA editing level. That is, the edit frequencies of all edit sites are drawn from the same beta distribution"
##################################
# This function returns the Maximum likelihood estimation of beta binomial distribution parameters prob (mu) and rho. We already know n.

mle.custom.h0 <- function( ref, alt, debug = F ){
  x <- alt # number reads supporting alternative allele (A to G or T to C)
  size <- ref + alt # total number of reads mapping at that position
  
  # Step 1: Construct the model: bbll is a function that returns the beta binomial density function of our data
  bbll <- function( prob, rho ){ # beta-binomial log likelihood 
    if( ( prob > 0 ) & ( rho > 0 ) & ( prob < 1 ) & ( rho < 1 ) ) # Note that 0<p<1 and 0<rho<1
      -sum( dbetabinom( x, size, prob, rho, log = T ) ) # dbetabinom(): Density function generator for the beta-binomial function, parameterized by n (size), probability (prob) and overdispersion (rho) (usage: dbetabinom(x, prob, size,  theta, shape1, shape2, log = FALSE)), where x is a numeric vector of integer values. For -sum(), we are taking the negative of the bb distribution (used for fit below)
    else NA # prob and rho do not fall in required range
  }
  
  # Step 2: Estimate the model parameters: Maximum likelihood estimation of parameters in beta binomial distribution
  # mle2(): Estimate parameters by the method of maximum likelihood.
  fit <- mle2( bbll, # function to calculate log likelihood for. Includes the parameters we are optimising
               start = list( prob = .1, rho = .5 ), # initial values of parameters for mle optimisation
               method = "Nelder-Mead", # a method of optimisation
               control = list( maxit = 1e5, trace = as.integer(debug) ) ) # should be some option for debugging
  return( fit )
}

# mle2() function arguments: 
#mle2(minuslogl, start, method, optimizer,
#     fixed = NULL, data=NULL,
#     subset=NULL,
#     default.start=TRUE, eval.only = FALSE, vecpar=FALSE,
#     parameters=NULL,
#     parnames=NULL,
#     skip.hessian=FALSE,
#     hessian.opts=NULL,
#     use.ginv=TRUE,
#     trace=FALSE,
#     browse_obj=FALSE,
#     gr=NULL,
#     optimfun,.)

##################################
# Generating our model for alt hypothesis
# MLE for our Alternative hypothesis: 
# H1 (ALT HYPOTHESIS): "Samples with fusion protein have different edit frequencies than control"
##################################

mle.custom.h1 <- function( ref1, alt1, ref2, alt2, debug = F ){
  x1 <- alt1 # counts of alt allele in control sample
  size1 <- ref1 + alt1 # total counts of alt + ref alleles at that site
  prob1.init <- mean(x1/size1)  + 1e-3 # initial probability of an A to G or T to C change
  if( is.na(prob1.init) | prob1.init >= 1 | prob1.init <= 0 ){
    prob1.init <- .05 # if there are cases where the previous line returns an NA, set initial probability at 0.05
  } 
  x2 <- alt2 # counts of alt allele in FP sample
  size2 <- ref2 + alt2 # total counts of alt + ref alleles at that site
  prob2.init <- mean(x2/size2)  + 1e-3
  if( is.na(prob2.init) | prob2.init >= 1 | prob2.init <= 0 ){
    prob2.init <- .05
  } 
  
  bbll <- function( prob1, prob2, rho ){
    if( ( prob1 > 0 ) & ( prob2 > 0 ) & ( rho > 0 ) &
        ( prob1 < 1 ) & ( prob2 < 1 ) & ( rho < 1 ) ) # If 0<prob1<1, 0<prob2<1, 0<rho<1
      -( sum( dbetabinom( x1, size1, prob1, rho, log = T ) ) + 
           sum( dbetabinom( x2, size2, prob2, rho, log = T ) ) ) # generate 2 different beta binomial distributions (1 for control, 1 for FP)
    else NA 
  }
  
  # Estimate the parameters of beta binomial distribution defined above
  fit <- mle2( bbll,
               start = list( prob1 = prob1.init, prob2 = prob2.init, rho = .1 ),
               method = "Nelder-Mead", # hessian = FALSE, 
               control = list( maxit = 1e5, trace = as.integer(debug) ) ) 
  return( fit )
}

##################################
# Now that we have the models for both null and alt hypotheses, we can use Likelihood ratio test (not to be confused with Maximum likelihood estimation) to test which model (H0 or H1) fits our data better 
##################################

# Easy explanation of Likelihood ratio test: https://api.rpubs.com/tomanderson_34/lrt#:~:text=Likelihood%20ratio%20tests%20are%20used,your%20model%20significantly%20more%20accurate. 

# Or for the lazy, it is also repeated here:

# Likelihood ratio tests are used to compare the goodness of fit of two statistical models. The LRT compares two hierarchically nested models to determine whether or not adding complexity to your model (i.e., adding more parameters) makes your model significantly more accurate. The “hierarchically nested models” simply means that the complex model differs only from the simpler (or “nested”) model by the addition of one or more parameters.

# In their most basic form, the hypotheses for the LRT are:

# H0: You should use the nested model.
# Ha: You should use the complex model.
# Thus, if you reject the H0, you can conclude that the complex model is significantly more accurate than the nested model, and you would choose to use the complex model. If you fail to reject the H0, you can conclude that the complex model is NOT significantly more accurate than the nested model, so you would choose to use the nested model instead.

# The test statistic for the LRT follows a chi-squared distribution with degrees of freedom equal to the difference in dimensionality of your models. The equation for the test statistic is provided below:
# -2 * [loglikelihood(nested)-loglikelihood(complex)]


# Counts of all reference and alternate allele counts
ref.counts <- df_snp_counts[,grep('ref.count', colnames(df_snp_counts))] # grab all the columns with ".ref.count" and their values
alt.counts <- df_snp_counts[,grep('alt.count', colnames(df_snp_counts))] # grab all the columns with ".alt.count" and their values

# Extract paired control and FP sample !!!!!!!!!!Adjust this part base on the experimental design
cur_df_snp_counts <- df_snp_counts %>%
  select(-A3_dedupped.bam.ref.count, -A3_dedupped.bam.alt.count, -AO7_dedupped.bam.ref.count, -AO7_dedupped.bam.alt.count) %>%
  filter(rowSums(select(., contains('count'))) != 0)

# About 15min
# LRT testing function with added checks and print statements for debugging
lrt.res <- mclapply(1:nrow(cur_df_snp_counts), function(i) {
  counts <- as.integer(cur_df_snp_counts[i, -(1:8)])  # grab each row for the last 9 columns
  ref <- counts[seq(1, length(counts), 2)]  # extract ref counts
  alt <- counts[seq(2, length(counts), 2)]  # extract alt counts
  
  # Add print statements to track the progress and data
  print(paste("Processing row:", i))
  print(paste("ref:", paste(ref, collapse = ",")))
  print(paste("alt:", paste(alt, collapse = ",")))
  
  # Fit the null model (H0: Same beta-binomial distribution for all samples)
  fit0 <- tryCatch({
    mle.custom.h0(ref, alt)
  }, error = function(e) {
    print(paste("Error in mle.custom.h0 for row:", i, "->", e))
    return(NULL)
  })
  
  # Fit the alternative model (H1: Different distributions between control and FP)
  fit1 <- tryCatch({
    mle.custom.h1(ref[1:2], alt[1:2], ref[-(1:2)], alt[-(1:2)])
  }, error = function(e) {
    print(paste("Error in mle.custom.h1 for row:", i, "->", e))
    return(NULL)
  })
  
  # If either fit fails or is NULL, return NA for this row
  if (is.null(fit0) || is.null(fit1)) {
    return(c(p.value = NA, value = NA, code = NA))
  }
  
  # Perform likelihood ratio test only if both models are valid
  print(paste0("fit0@min: ", fit0@min ,"fit1@min: ", fit1@min))
  if (!is.na(fit0@min) && !is.na(fit1@min)) {
    p.value <- pchisq(2 * (fit0@min - fit1@min), 1, lower.tail = FALSE)
    value <- fit1@min
    code <- fit1@details$convergence  # Check if optimization converged
    return(c(p.value = p.value, value = value, code = code))
  } else {
    return(c(p.value = NA, value = NA, code = NA))
  }
  
}, mc.cores = 1)

# Convert results to a data frame
lrt.res <- data.frame(do.call('rbind', lrt.res))



# Additional columns
alt.freq <- alt.counts / ( ref.counts + alt.counts ) # for all rows, obtaining edit frequency --> alt/(alt+ref)
df.stats <- data.frame( 
  diff.frequency = rowMeans(alt.freq[,c(4,5,6)],na.rm=T) - rowMeans(alt.freq[,-c(4,5,6)],na.rm=T), # btwn mean of ADAR and mean of DCD+MIG
  DCD.frequency = rowMeans( alt.freq[,c(1,2,3)],na.rm=T ), 
  ADA.frequency = rowMeans( alt.freq[,c(4,5,6)],na.rm=T ),
  MIG.frequency = rowMeans( alt.freq[,c(7,8,9)],na.rm=T ) )

df.stats <- cbind( df.stats, lrt.res ) # combine both dataframes


# Do some filtering of genes
# I have no idea what weird filtering this is...?! But it kills like 3205 of 17659 of my SNPs
rowmask <- rowSums( alt.counts != 0 ) > 1 & # for those alt counts > 0, sum up the rows
  ( rowMeans( ref.counts[4:5] ) + rowMeans( alt.counts[4:5] ) >= 5 ) &
  ( rowMeans( ref.counts ) + rowMeans( alt.counts ) >= 5 ) &
  lrt.res$value > 0 & 
  !is.na(cur_df_snp_counts$gene.symbol) # has a gene symbol

# Anyway let's just continue first
df.stats.test <- df.stats[ rowmask, ]
df.stats.test$p.adj <- p.adjust( df.stats.test$p.value, 'BH' ) # Adjust p value to control for false discovery rate (FDR) using a Benjamin–Hochberg correction (FDR = FP / (FP + TP)), FDR wants to reduce false positives
df.stats.test$value <- NULL # replace all the values with NULL
df.stats.test$code <- NULL # replace all the codes with NULL
df1 <- cbind( cur_df_snp_counts[ rowmask, 1:8 ], df.stats.test, cur_df_snp_counts[ rowmask, -(1:8) ] ) # generating huge dataframe with all the info so far

# Filtering for snps with diff.frequency> 0 and adjusted pvalue < 0.05
df1 <- df1[ which( df1$diff.frequency > 0 & df1$p.adj < .05 ), ]
